{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2496734d-9fd1-4b00-a90f-a7e47bca9378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3472a868-d9de-4e3c-aa33-848b80ff04a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\jayan\\appdata\\roaming\\python\\python311\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b476cdb-42b4-47c3-b5ce-79bf3a6a8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sentencepieceNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Version: 0.2.0\n",
      "Summary: SentencePiece python wrapper\n",
      "Home-page: https://github.com/google/sentencepiece\n",
      "Author: Taku Kudo\n",
      "Author-email: taku@google.com\n",
      "License: Apache\n",
      "Location: C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "pip show sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d60ce825-21bd-40aa-9126-792a66a05af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "print(\"SentencePiece imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30263080-b0e4-4c65-87b4-0ccdeb09851c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "try:\n",
    "    tokenizer=T5Tokenizer.from_pretrained('t5-small')\n",
    "    print(\"T5Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b4c4ae-ff86-482e-b3f1-f7ae8e597268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How2sign  dataset\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "import numpy as np #importing necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class How2signDataset:\n",
    "    def __init__(self, json_files, csv_file, seq_len=183, time_len=512):\n",
    "        self.seq_len = seq_len\n",
    "        self.time_len = time_len\n",
    "        self.json_files = json_files\n",
    "        self.csv_file = csv_file\n",
    "\n",
    "        self.sentence_dict = self.load_y()\n",
    "\n",
    "    def get_x(self, x_path):\n",
    "        # Load the JSON data\n",
    "        with open(x_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract the hand_pose_face keypoints\n",
    "        hand_pose_faces = [person['hand_pose_face'] for person in data['people']]\n",
    "\n",
    "        hand_pose_faces = np.array(hand_pose_faces)\n",
    "        hand_pose_faces = hand_pose_faces.reshape(1, -1, self.seq_len)\n",
    "\n",
    "        # Create an array to store the padded data\n",
    "        x = np.zeros((1, self.time_len, self.seq_len))\n",
    "\n",
    "        # Fill the padded array with the actual data\n",
    "        seq_length = len(hand_pose_faces[0])\n",
    "        x[:, :seq_length, :] = hand_pose_faces[:, :seq_length, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def load_y(self):\n",
    "        data = pd.read_csv(self.csv_file, delimiter='\\t', on_bad_lines='skip')\n",
    "        df = data[['SENTENCE_NAME', 'SENTENCE']]\n",
    "        sentence_dict = pd.Series(df.SENTENCE.values, index=df.SENTENCE_NAME).to_dict()\n",
    "\n",
    "        return sentence_dict\n",
    "\n",
    "    def get_y(self, x_base_path):\n",
    "        y = self.sentence_dict.get(x_base_path, \"0\")\n",
    "\n",
    "        return y\n",
    "\n",
    "    def how2sign_keypoints_sentence(self):\n",
    "        # Load the data from multiple files\n",
    "        x = [self.get_x(json_file) for json_file in self.json_files]\n",
    "        x = np.concatenate(x, axis=0)\n",
    "\n",
    "        json_files_base = [json_file.split(\".\")[0] for json_file in self.json_files]\n",
    "        print(json_files_base)\n",
    "        y = [self.get_y(json_file_base) for json_file_base in json_files_base]\n",
    "        y = np.array(y)\n",
    "\n",
    "        # Concatenate the data from the files\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e769e0c-7361-4a89-a0e1-89b091a68649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CO6qyvvglAE_18-5-rgb_front']\n",
      "[[[0.64002734 0.63365875 0.639195   ... 0.5492125  0.30881625 0.887098  ]\n",
      "  [0.64035156 0.62237875 0.580242   ... 0.55052266 0.30683125 0.922378  ]\n",
      "  [0.63711953 0.59722625 0.608995   ... 0.55191484 0.30503375 0.947329  ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
      "['Now a great feature of the Denon CD player is, if you decide this is the track you want, instead of hitting pause, which just pauses it, or hitting stop, which actually would bring you back to track one, you can hit, go find the track you want again, track three in this case.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "(1, 512, 183)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "json_files = [\"CO6qyvvglAE_18-5-rgb_front.json\"]\n",
    "csv_file = \"how2sign_realigned_val.csv\"\n",
    "\n",
    "# json_files = find_files(\"data/how2sign/realigned_val\", pattern='**/*.json', interval=1)\n",
    "dataset = How2signDataset(json_files=json_files, csv_file=csv_file)\n",
    "x, y = dataset.how2sign_keypoints_sentence()\n",
    "\n",
    "print(x)\n",
    "print(y) \n",
    "print('-'*100)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c1ee38-b3ab-4f33-ae58-95f210f1f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearProjectionWithConv(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(LinearProjectionWithConv, self).__init__()\n",
    "        # Kernel size is (183, 1) and stride is (183, 1)\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=embed_dim, kernel_size=(183, 1), stride=(183, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 183, 1024)\n",
    "        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, 183, 1024)\n",
    "        x = self.conv(x)  # Apply convolution\n",
    "        x = x.squeeze(2)  # Remove the channel dimension: (batch_size, embed_dim, new_sequence_length)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc467f07-f12c-49e6-9385-c7ae382ae3c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'how2sign_keypoints_sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m test_data\u001b[38;5;241m=\u001b[39mfind_files(test_DATA)\n\u001b[0;32m     73\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m How2signDataset(test_data,csv_file)\n\u001b[1;32m---> 74\u001b[0m x_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow2sign_keypoints_sentence\u001b[49m()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_test\u001b[38;5;241m.\u001b[39mshape, y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     79\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'how2sign_keypoints_sentence'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def  find_files(directory, pattern='**/*.json'):\n",
    "    return glob(os.path.join(directory,pattern),recursive=True)\n",
    "\n",
    "'''# Define paths to your folders\n",
    "train_folder = \"J:\\\\train_2D_keypoints\\\\openpose_output\\\\output\"\n",
    "val_folder = \"J:\\\\val_2D_keypoints\\\\openpose_output\\\\output\"\n",
    "test_folder = \"J:\\\\test_2D_keypoints\\\\openpose_output\\\\output\" '''\n",
    "\n",
    "'''# Load the datasets\n",
    "train_data = find_files(train_folder)\n",
    "val_data = find_files(val_folder)\n",
    "test_data = find_files(test_folder)'''\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer and Dataset Preparation\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "\n",
    "\n",
    "'''class How2signDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze()\n",
    "        } '''\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "#y data\n",
    "train_DATA= \"J:\\\\train_2D_keypoints\\\\openpose_output\\\\output\"\n",
    "csv_file = \"J:\\Folders\\Downloads\\how2sign_realigned_train.csv\"\n",
    "train_data=find_files(train_DATA)\n",
    "# train_dataset = How2signDataset(train_data,csv_file)\n",
    "# x_train, y_train = train_data.how2sign_keypoints_sentence()\n",
    "# print(x_train.shape, y_train.shape)\n",
    "\n",
    "val_DATA= \"J:\\\\val_2D_keypoints\\\\openpose_output\\\\output\"\n",
    "csv_file = \"J:\\Folders\\Downloads\\how2sign_realigned_val.csv\"\n",
    "val_data=find_files(val_DATA)\n",
    "# val_dataset = How2signDataset(val_data, csv_file)\n",
    "# x_val, y_val = val_data.how2sign_keypoints_sentence()\n",
    "# print(x_val.shape, y_val.shape)\n",
    "\n",
    "\n",
    "test_DATA = \"J:\\\\test_2D_keypoints\\\\openpose_output\\\\output\"\n",
    "csv_file = \"J:\\Folders\\Downloads\\how2sign_realigned_test.csv\"\n",
    "test_data=find_files(test_DATA)\n",
    "test_dataset = How2signDataset(test_data,csv_file)\n",
    "x_test, y_test = test_data.how2sign_keypoints_sentence()\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0746209-ec88-4e00-a37f-75504921bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class T5WithProjection(nn.Module):\n",
    "    def __init__(self, t5_model_name, embed_dim):\n",
    "        super(T5WithProjection, self).__init__()\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "        self.projection = LinearProjectionWithConv(embed_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "         # Apply linear projection\n",
    "        input_ids = self.projection(outputs.logits)  # Apply projection to logits\n",
    "        # Forward pass through T5 model\n",
    "        outputs = self.t5(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "       \n",
    "        return outputs\n",
    "\n",
    "# Initialize the model with projection\n",
    "embed_dim = 1  # Adjust according to your T5 model's hidden size\n",
    "model = T5WithProjection('t5-small', embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ee4eaf-01ac-487b-9bab-0c33b3023ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\jayan\\AppData\\Local\\Temp\\ipykernel_7204\\3454902787.py\", line 2, in <module>\n",
      "    from transformers import AdamW\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py\", line 1593, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py\", line 1603, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py\", line 27, in <module>\n",
      "    from .trainer_pt_utils import LayerWiseDummyOptimizer, LayerWiseDummyScheduler\n",
      "  File \"C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer_pt_utils.py\", line 231, in <module>\n",
      "    device: Optional[torch.device] = torch.device(\"cuda\"),\n",
      "C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer_pt_utils.py:231: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n",
      "C:\\Users\\jayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TextDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Define your dataset and dataloader\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m  \u001b[43mTextDataset\u001b[49m(tokenizer,max_len,max_len)  \n\u001b[0;32m     29\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Define the training function\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextDataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import T5ForConditionalGeneration, AdamW, T5Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "'''# test\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"example sentence\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "print(input_ids)\n",
    "print(labels)'''\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define your dataset and dataloader\n",
    "train_dataset =  How2signDataset(tokenizer,max_len,max_len)  \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the training function\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = input_ids.view(-1)\n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "    print(f\"Epoch {epoch + 1}: Training loss = {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ee6038-5c60-4f12-aca2-d853cbc600d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-with-projection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-with-projection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('t5-with-projection')\n",
    "tokenizer.save_pretrained('t5-with-projection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48faac-b0f6-4b7b-b5db-1b0562036c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6adc25-a4c3-441f-a0a3-d3dd7dfe193c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81db2b5-02b3-45b3-ba84-44e2a5666c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421fda8-1fe9-409b-a060-2f3e4153928a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
